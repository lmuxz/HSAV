{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from io_utils import load_dataset, model_log\n",
    "from metric import performance_logloss, performance_pr_auc\n",
    "\n",
    "\n",
    "from labelshift_correction import build_pivot_dataset\n",
    "from train_utils import sample_validation_data, extend_dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from subdomain_tools import sub_domain_ind\n",
    "\n",
    "from DCTN import embed_nn, dctn_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"kaggle\"\n",
    "model_type = \"nn\" \n",
    "\n",
    "num_dim = 43\n",
    "period = [0, 1, 2]\n",
    "cate_index = 8\n",
    "embedding_input = [3, 131, 4, 483, 103, 5, 106, 4]\n",
    "embedding_dim = [1, 3, 1, 4, 3, 1, 3, 1]\n",
    "\n",
    "test_flag = False\n",
    "\n",
    "n_label = 200\n",
    "\n",
    "train_mode = \"sup\"\n",
    "# train_mode = \"unsup\"\n",
    "\n",
    "version = \"exp_dctn_\" + train_mode\n",
    "\n",
    "source_version = \"uni\" \n",
    "data_type = \"uni\"\n",
    "source_domain = \"source\"\n",
    "target_domain = \"target\"\n",
    "njobs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_spoints = {\n",
    "    1: [],\n",
    "    2: [12814],\n",
    "    3: [12201, 37891]\n",
    "}\n",
    "\n",
    "\n",
    "target_spoints_p = {\n",
    "    0: {\n",
    "        1: [],\n",
    "        2: [10507],\n",
    "        # not possible to get 3 subdomains\n",
    "    },\n",
    "    1: {\n",
    "        1: [],\n",
    "        2: [3303],\n",
    "        # not possible to get 3 subdomains\n",
    "    },\n",
    "    2: {\n",
    "        1: [],\n",
    "        2: [18278],\n",
    "        3: [5965, 16411],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for seed in range(10):\n",
    "    for p in period:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        print(\"Period:\", p, seed, flush=True)\n",
    "        \n",
    "        # load source and target data\n",
    "        source_train, source_train_label, source_test, source_test_label = load_dataset(\"../data/\", \n",
    "                                                                                        task, source_domain, data_type, 0)\n",
    "        target_train, target_train_label, target_test, target_test_label = load_dataset(\"../data/\", \n",
    "                                                                                        task, target_domain, data_type, p)\n",
    "        \n",
    "        # set time order to label first index\n",
    "        source_train_label[:, 0] = np.arange(source_train_label.shape[0])\n",
    "        target_train_label[:, 0] = np.arange(target_train_label.shape[0])\n",
    "        \n",
    "        \n",
    "        # sample target supervised examples\n",
    "        target_train_index, sample_label = sample_validation_data(task, target_train_label, \n",
    "                                                                  ratio=1.0, number_examples=n_label)\n",
    "        target_sample = target_train[target_train_index]\n",
    "        target_sample_label = target_train_label[target_train_index]\n",
    "\n",
    "        \n",
    "        kt = 1\n",
    "        ks = 2\n",
    "        \n",
    "        source_subdata, source_sublabel = sub_domain_ind(source_train, source_train_label, source_spoints[ks])\n",
    "        \n",
    "        for j in range(ks):\n",
    "            # get target_factor and source_factor\n",
    "            source_factor = (source_sublabel[j][:, 1]==0).sum() / source_sublabel[j][:, 1].sum()\n",
    "            target_factor = (target_train_label[:, 1]==0).sum() / target_train_label[:, 1].sum()\n",
    "\n",
    "            # adjusting source train dataset\n",
    "            source_train_sub, source_train_label_sub, source_index = build_pivot_dataset(\n",
    "                source_subdata[j], source_sublabel[j], target_factor, source_factor)\n",
    "            \n",
    "            source_subdata[j] = source_train_sub\n",
    "            source_sublabel[j] = source_train_label_sub\n",
    "        \n",
    "        \n",
    "        xs, xv, ys, yv = train_test_split(\n",
    "            source_subdata[0], source_sublabel[0][:, 1], test_size=0.25, shuffle=True, random_state=seed)\n",
    "\n",
    "        xss, xvv, yss, yvv = train_test_split(\n",
    "            source_subdata[1], source_sublabel[1][:, 1], test_size=0.25, shuffle=True, random_state=seed)\n",
    "        \n",
    "        source_index, target_index = extend_dataset(xs, xss)\n",
    "        xs, ys = xs[source_index], ys[source_index]\n",
    "        xss, yss = xss[target_index], yss[target_index]\n",
    "        \n",
    "        source_index, target_index = extend_dataset(xv, xvv)\n",
    "        xv, yv = xv[source_index], yv[source_index]\n",
    "        xvv, yvv = xvv[target_index], yvv[target_index]\n",
    "        \n",
    "        \n",
    "        source_perfs = []\n",
    "        target_perfs_logloss = []\n",
    "        target_perfs_prauc = []\n",
    "        lmbda = 0.01 \n",
    "        for lr in [0.001, 0.003, 0.005, 0.007, 0.01]:\n",
    "\n",
    "            pre_train_params = {\"epoch\": 25, \"batch_size\": 1024, \"lr\": lr}\n",
    "            multi_adapt_params = {\"max_iter\": 25, \"max_beta\": 20, \"batch_size\": 512, \"pos_lmbda\": 0.95, \"neg_lmbda\": 0.0005, \n",
    "                                  \"lmbda\": lmbda, \"tol\": 1e-3, \"lr\": lr}\n",
    "\n",
    "            embed = embed_nn(embedding_input, embedding_dim, num_dim)\n",
    "            dctn = dctn_model(embed, cate_index, torch.device(\"cuda\"))\n",
    "\n",
    "            if train_mode == \"unsup\":\n",
    "                dctn.fit(xs, ys, xss, yss, \n",
    "                         target_train, \n",
    "                         xv, yv, xvv, yvv, \n",
    "                         None, None, \n",
    "                         pre_train_params, multi_adapt_params, \n",
    "                         early_stop=False, verbose=False)\n",
    "            elif train_mode == \"sup\":\n",
    "                dctn.fit(xs, ys, xss, yss, \n",
    "                         target_train, \n",
    "                         xv, yv, xvv, yvv, \n",
    "                         target_sample, target_sample_label[:,1], \n",
    "                         pre_train_params, multi_adapt_params, \n",
    "                         early_stop=False, verbose=False)\n",
    "            else:\n",
    "                print(\"Unexpected parameters\", flush=True)\n",
    "                raise \n",
    "\n",
    "                \n",
    "            pred = dctn.predict(source_test)\n",
    "            \n",
    "            perf = performance_pr_auc(pred, source_test_label[:, 1])\n",
    "            print(\"Source Prediction pr_auc:\", perf, flush=True)\n",
    "            source_perfs.append(perf)\n",
    "            \n",
    "            if train_mode == \"sup\":\n",
    "                pred = dctn.predict(target_sample)\n",
    "                perf = performance_pr_auc(pred, target_sample_label[:,1])\n",
    "                source_perfs[-1] = source_perfs[-1] + perf * lmbda\n",
    "            \n",
    "\n",
    "            # target prediction\n",
    "            pred = dctn.predict(target_test)\n",
    "\n",
    "            perf = performance_logloss(pred, target_test_label[:, 1])\n",
    "            target_perfs_logloss.append(perf)\n",
    "            print(\"Target Prediction logloss:\", perf, flush=True)\n",
    "\n",
    "            perf = performance_pr_auc(pred, target_test_label[:, 1])\n",
    "            target_perfs_prauc.append(perf)\n",
    "            print(\"Target Prediction pr_auc:\", perf, flush=True)\n",
    "\n",
    "        \n",
    "        print(\"Source performances:\", source_perfs)\n",
    "        print(\"Target performances Logloss:\", target_perfs_logloss)\n",
    "        print(\"Target performances pr_auc:\", target_perfs_prauc)\n",
    "        opt_ind = np.argmax(source_perfs)\n",
    "        print(\"Target performances :\", target_perfs_prauc[opt_ind])\n",
    "        if not test_flag:\n",
    "            \n",
    "            model_log(\"../logs/logloss/\", task, source_domain, model_type, p, source_version, \n",
    "                     \"{}: {}\".format(version, target_perfs_logloss[opt_ind]))\n",
    "            model_log(\"../logs/pr_auc/\", task, source_domain, model_type, p, source_version, \n",
    "                     \"{}: {}\".format(version, target_perfs_prauc[opt_ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
