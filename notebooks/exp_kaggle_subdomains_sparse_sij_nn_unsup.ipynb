{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from io_utils import load_dataset, load_model, model_log, save_pickle\n",
    "from metric import performance_logloss, performance_pr_auc\n",
    "\n",
    "from labelshift_correction import build_pivot_dataset, adjust_model\n",
    "from train_utils import sample_validation_data, LogScore\n",
    "\n",
    "from coordinate_ot_adaptation_acc import adaptation, Discretize\n",
    "\n",
    "from feature_selection_sup import feature_selection_sup\n",
    "from feature_selection_acc import feature_selection\n",
    "\n",
    "from subdomain_tools import sub_domain_ind, get_pi, pred_ij, weight_pred, \\\n",
    "    sij_minimization_sup, sij_minimization_unsup, \\\n",
    "    optimal_kts_unsup, optimal_kts_unsup_reg, optimal_counter_weight_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"kaggle\"\n",
    "model_type = \"nn\"\n",
    "\n",
    "num_dim = 43\n",
    "period = [0, 1, 2]\n",
    "cate_index = 8\n",
    "\n",
    "test_flag = False\n",
    "\n",
    "n_label = 200\n",
    "n_source_pred = 10000\n",
    "n_pseudo = 10000\n",
    "\n",
    "# train_mode = \"sup\"\n",
    "train_mode = \"unsup\"\n",
    "\n",
    "version = \"exp_subunsup_sparse\"\n",
    "basic_version = \"exp_unsup_sparse\"\n",
    "\n",
    "\n",
    "source_version = \"uni\" \n",
    "data_type = \"uni\"\n",
    "source_domain = \"source\"\n",
    "target_domain = \"target\"\n",
    "njobs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_spoints = {\n",
    "    1: [],\n",
    "    2: [12814],\n",
    "    3: [12201, 37891]\n",
    "}\n",
    "\n",
    "\n",
    "target_spoints_p = {\n",
    "    0: {\n",
    "        1: [],\n",
    "        2: [10507],\n",
    "        # not possible to get 3 subdomains\n",
    "    },\n",
    "    1: {\n",
    "        1: [],\n",
    "        2: [3303],\n",
    "        # not possible to get 3 subdomains\n",
    "    },\n",
    "    2: {\n",
    "        1: [],\n",
    "        2: [18278],\n",
    "        3: [5965, 16411],\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "for seed in range(10):\n",
    "    for p in period:\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        print(\"Period:\", p, seed, flush=True)\n",
    "        \n",
    "        # load source and target data\n",
    "        source_train, source_train_label, source_test, source_test_label = load_dataset(\"../data/\", \n",
    "                                                                                        task, source_domain, data_type, 0)\n",
    "        target_train, target_train_label, target_test, target_test_label = load_dataset(\"../data/\", \n",
    "                                                                                        task, target_domain, data_type, p)\n",
    "        \n",
    "        # discretize\n",
    "        source_train_num = source_train[:, cate_index:]\n",
    "        target_train_num = target_train[:, cate_index:]\n",
    "\n",
    "        source_train_cate = source_train[:, :cate_index]\n",
    "        target_train_cate = target_train[:, :cate_index]\n",
    "\n",
    "        discret = Discretize()\n",
    "        discret.fit(np.vstack([source_train_num, target_train_num]))\n",
    "\n",
    "        source_train_num = discret.transform(source_train_num)\n",
    "        target_train_num = discret.transform(target_train_num)\n",
    "\n",
    "        source_train = np.hstack([source_train_cate, source_train_num])\n",
    "        target_train = np.hstack([target_train_cate, target_train_num])\n",
    "        \n",
    "        \n",
    "        # set time order to label first index\n",
    "        source_train_label[:, 0] = np.arange(source_train_label.shape[0])\n",
    "        target_train_label[:, 0] = np.arange(target_train_label.shape[0])\n",
    "\n",
    "\n",
    "        # get source reference prediction\n",
    "        model = load_model(\"../model/\", task, source_domain, model_type, 0, source_version)\n",
    "        # get target_factor and source_factor\n",
    "        source_factor = (source_train_label[:, 1]==0).sum() / source_train_label[:, 1].sum()\n",
    "        target_factor = (target_train_label[:, 1]==0).sum() / target_train_label[:, 1].sum()\n",
    "\n",
    "        # adjusting the classifier\n",
    "        model = adjust_model(model, target_factor, source_factor)\n",
    "\n",
    "        # source sample\n",
    "        source_train_index, sample_label = sample_validation_data(task, source_train_label, \n",
    "                                                                  ratio=1.0, number_examples=n_source_pred)\n",
    "        source_sample = source_train[source_train_index]\n",
    "        source_pred = model.predict(source_sample)\n",
    "\n",
    "\n",
    "        # sample target supervised examples\n",
    "        target_train_index, sample_label = sample_validation_data(task, target_train_label, \n",
    "                                                                  ratio=1.0, number_examples=n_label)\n",
    "        target_sample = target_train[target_train_index]\n",
    "        target_sample_label = target_train_label[target_train_index]\n",
    "\n",
    "\n",
    "        # sample target pseudo label data\n",
    "        pseudo_mask = np.zeros(target_train.shape[0])\n",
    "        under_s = np.random.choice(len(pseudo_mask), n_pseudo, replace=False)\n",
    "\n",
    "        pseudo_mask[under_s] = 1\n",
    "\n",
    "        target_pseudo = target_train[pseudo_mask.astype(bool)]\n",
    "        target_pseudo_label = target_train_label[pseudo_mask.astype(bool)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        pscore_t = {} # for pi computation\n",
    "        adapt_ts = defaultdict(dict) # for trans_ij computation\n",
    "        fmask_ts = defaultdict(dict) # for feature of trans_ij computation\n",
    "        model_ts = defaultdict(dict) # for adapted model\n",
    "        sij_ts = defaultdict(dict) # for sij computation      \n",
    "\n",
    "        for kt in target_spoints_p[p]:\n",
    "            print(\"##### Target kt:\", kt, flush=True)\n",
    "                \n",
    "            # get target sub domains\n",
    "            target_subdata, target_sublabel = sub_domain_ind(target_train, target_train_label, target_spoints_p[p][kt])\n",
    "\n",
    "            # get sample target data subdomains\n",
    "            target_spdata, target_splabel = sub_domain_ind(target_sample, target_sample_label, target_spoints_p[p][kt])\n",
    "            \n",
    "            # get pseudo target subdomain\n",
    "            target_pseudata, target_pseulabel = sub_domain_ind(target_pseudo, target_pseudo_label, target_spoints_p[p][kt])\n",
    "\n",
    "            \n",
    "            # init affectation of target examples\n",
    "            pscore = []\n",
    "            for i in range(kt):\n",
    "                logscore = LogScore(cate_index)\n",
    "                logscore.fit(target_subdata[i])\n",
    "                pscore.append(logscore)\n",
    "            \n",
    "            pscore_t[kt] = pscore\n",
    "\n",
    "            for ks in source_spoints:\n",
    "                print(\"## Source ks:\", ks, flush=True)\n",
    "                # get source sub domains\n",
    "                source_subdata, source_sublabel = sub_domain_ind(source_train, source_train_label, source_spoints[ks])\n",
    "\n",
    "                adapts = []\n",
    "                fmasks = []\n",
    "                models = []\n",
    "                \n",
    "                adapt_ts[kt][ks] = adapts\n",
    "                fmask_ts[kt][ks] = fmasks\n",
    "                model_ts[kt][ks] = models\n",
    "                \n",
    "                for i in range(kt):\n",
    "                    adapti = []\n",
    "                    adapts.append(adapti)\n",
    "                    \n",
    "                    fmaski = []\n",
    "                    fmasks.append(fmaski)\n",
    "                    \n",
    "                    modeli = []\n",
    "                    models.append(modeli)\n",
    "                    for j in range(ks):\n",
    "                        print(\"Adaptation i={}, j={}\".format(i+1, j+1))\n",
    "\n",
    "                        \n",
    "                        # load source model\n",
    "                        model = load_model(\"../model/\", task, source_domain, model_type, 0, source_version)\n",
    "\n",
    "                        # get target_factor and source_factor\n",
    "                        source_factor = (source_train_label[:, 1]==0).sum() / source_train_label[:, 1].sum()\n",
    "                        target_factor = (target_sublabel[i][:, 1]==0).sum() / target_sublabel[i][:, 1].sum()\n",
    "\n",
    "                        # adjusting the classifier\n",
    "                        model = adjust_model(model, target_factor, source_factor)\n",
    "                        modeli.append(model)\n",
    "\n",
    "                        # get target_factor and source_factor\n",
    "                        source_factor = (source_sublabel[j][:, 1]==0).sum() / source_sublabel[j][:, 1].sum()\n",
    "                        target_factor = (target_sublabel[i][:, 1]==0).sum() / target_sublabel[i][:, 1].sum()\n",
    "\n",
    "                        # adjusting source train dataset\n",
    "                        source_train_sub, source_train_label_sub, source_index = build_pivot_dataset(\n",
    "                            source_subdata[j], source_sublabel[j], target_factor, source_factor)\n",
    "                        \n",
    "                        \n",
    "                        # source subdomain prediction\n",
    "                        source_pred_sub = model.predict(source_train_sub)\n",
    "                        \n",
    "                        # adaptation\n",
    "                        adapt = adaptation(cate_dim=cate_index, num_dim=num_dim)\n",
    "                        adapt.fit(target_subdata[i], source_train_sub, lmbda=1e-1)\n",
    "                        adapti.append(adapt)\n",
    "\n",
    "                        target_train_trans = adapt.transform(target_spdata[i], repeat=5, njobs=njobs)\n",
    "                        targert_sub_trans = adapt.transform(target_pseudata[i], repeat=5, njobs=njobs)\n",
    "                        \n",
    "\n",
    "                        feature_selection_process = None\n",
    "                        if train_mode == \"sup\":\n",
    "                            # supervised feature selection\n",
    "                            params = {\n",
    "                                \"model\": model, \n",
    "                                \"valid\": target_spdata[i],\n",
    "                                \"valid_trans\": target_train_trans,\n",
    "                                \"ref_label\": target_splabel[i][:, 1],\n",
    "                                \"repeat\": 5,\n",
    "                                \"n_bootstrap\": 100,\n",
    "                                \"njobs\": 20,\n",
    "                                \"bootstrap_tol\": 0.2,\n",
    "                                \"max_feature\": 4,\n",
    "                                \"verbose\": False,\n",
    "                            }\n",
    "                            feature_selection_process = feature_selection_sup\n",
    "                            \n",
    "                        elif train_mode == \"unsup\":    \n",
    "                            # unsupervised feature selection\n",
    "                            params = {\n",
    "                                \"model\": model, \n",
    "                                \"valid\": target_pseudata[i],\n",
    "                                \"valid_trans\": targert_sub_trans,\n",
    "                                \"ref_label\": source_pred_sub,\n",
    "                                \"repeat\": 5,\n",
    "                                \"delta\": 0.05,\n",
    "                                \"n_bootstrap\": 100,\n",
    "                                \"njobs\": 20,\n",
    "                                \"bootstrap_tol\": 0.2,\n",
    "                                \"max_feature\": 4,\n",
    "                                \"verbose\": False,\n",
    "                            }\n",
    "                            feature_selection_process = feature_selection\n",
    "                            \n",
    "                        else:\n",
    "                            print(\"Unexpected parameters\", flush=True)\n",
    "                            raise \n",
    "\n",
    "                        if (kt, ks) == (1, 1):\n",
    "                            feature_mask = feature_selection_process(**params)\n",
    "                        else:\n",
    "                            feature_maks = 1\n",
    "                        fmaski.append(feature_mask)\n",
    "                \n",
    "                \n",
    "                pi = get_pi(pscore, target_sample)\n",
    "                predij = pred_ij(models, adapts, fmasks, target_sample, repeat=5, njobs=20)\n",
    "                \n",
    "                pi_all = get_pi(pscore, target_train)\n",
    "                predij_all = pred_ij(models, adapts, fmasks, target_train, repeat=5, njobs=20)\n",
    "                \n",
    "\n",
    "                if train_mode == \"sup\":\n",
    "                    sij_softmax = sij_minimization_sup(pi, predij, target_sample_label[:, 1], \n",
    "                                                       lr=100, max_iter=1000, n_bootstrap=10, tol=1e-4)\n",
    "                elif train_mode == \"unsup\":    \n",
    "                    sij_softmax = sij_minimization_unsup(pi_all, predij_all, source_pred,\n",
    "                                                         lr=100, max_iter=1000, n_bootstrap=10, tol=1e-4)\n",
    "                else:\n",
    "                    print(\"Unexpected parameters\", flush=True)\n",
    "                    raise \n",
    "                sij_ts[kt][ks] = sij_softmax\n",
    "                \n",
    "        domain_mapping = defaultdict(dict)\n",
    "        for kt in sij_ts:\n",
    "            for ks in sij_ts[kt]:\n",
    "                domain_mapping[kt][ks] = np.argmax(sij_ts[kt][ks].reshape((kt, ks)), axis=1)\n",
    "\n",
    "        \n",
    "        # save exp results\n",
    "        if not test_flag:\n",
    "            save_pickle(sij_ts, \"../data/results/{}/{}_{}_{}_{}_{}\".format(\n",
    "                task, model_type, version, p, seed, \"sij_ts\"))\n",
    "            save_pickle(domain_mapping, \"../data/results/{}/{}_{}_{}_{}_{}\".format(\n",
    "                task, model_type, version, p, seed, \"domain_mapping\"))\n",
    "            save_pickle(fmask_ts, \"../data/results/{}/{}_{}_{}_{}_{}\".format(\n",
    "                task, model_type, version, p, seed, \"fmask_ts\"))\n",
    "        \n",
    "        \n",
    "        adapt_map_ts = defaultdict(dict)\n",
    "        fmask_map_ts = defaultdict(dict)\n",
    "        model_map_ts = defaultdict(dict)\n",
    "        sij_ts = defaultdict(dict)\n",
    "        \n",
    "        pred_ts = defaultdict(dict)\n",
    "        pred_all_ts = defaultdict(dict)\n",
    "        for kt in domain_mapping:\n",
    "            print(\"##### Target kt:\", kt, flush=True)\n",
    "                \n",
    "            # get target sub domains\n",
    "            target_subdata, target_sublabel = sub_domain_ind(target_train, target_train_label, target_spoints_p[p][kt])\n",
    "\n",
    "            # get sample target data subdomains\n",
    "            target_spdata, target_splabel = sub_domain_ind(target_sample, target_sample_label, target_spoints_p[p][kt])\n",
    "            \n",
    "            # get pseudo target subdomain\n",
    "            target_pseudata, target_pseulabel = sub_domain_ind(target_pseudo, target_pseudo_label, target_spoints_p[p][kt])\n",
    "            \n",
    "            pscore = pscore_t[kt]\n",
    "            for ks in domain_mapping[kt]:\n",
    "                print(\"## Source ks:\", ks, flush=True)\n",
    "                # get source sub domains\n",
    "                source_subdata, source_sublabel = sub_domain_ind(source_train, source_train_label, source_spoints[ks])\n",
    "                \n",
    "                adapts = []\n",
    "                fmasks = []\n",
    "                models = []\n",
    "                \n",
    "                adapt_map_ts[kt][ks] = adapts\n",
    "                fmask_map_ts[kt][ks] = fmasks\n",
    "                model_map_ts[kt][ks] = models\n",
    "                \n",
    "                for i in range(kt):\n",
    "                    j = domain_mapping[kt][ks][i]\n",
    "                    \n",
    "                    \n",
    "                    model = model_ts[kt][ks][i][j]\n",
    "                    models.append([model])\n",
    "                    adapt = adapt_ts[kt][ks][i][j]\n",
    "                    adapts.append([adapt])\n",
    "\n",
    "                    # get target_factor and source_factor\n",
    "                    source_factor = (source_sublabel[j][:, 1]==0).sum() / source_sublabel[j][:, 1].sum()\n",
    "                    target_factor = (target_sublabel[i][:, 1]==0).sum() / target_sublabel[i][:, 1].sum()\n",
    "\n",
    "                    # adjusting source train dataset\n",
    "                    source_train_sub, source_train_label_sub, source_index = build_pivot_dataset(\n",
    "                        source_subdata[j], source_sublabel[j], target_factor, source_factor)\n",
    "\n",
    "                    # source subdomain prediction\n",
    "                    source_pred_sub = model.predict(source_train_sub)\n",
    "\n",
    "                    # adaptation\n",
    "                    target_train_trans = adapt.transform(target_spdata[i], repeat=5, njobs=njobs)\n",
    "                    targert_sub_trans = adapt.transform(target_pseudata[i], repeat=5, njobs=njobs)\n",
    "        \n",
    "        \n",
    "                    feature_selection_process = None\n",
    "                    if train_mode == \"sup\":\n",
    "                        # supervised feature selection\n",
    "                        params = {\n",
    "                            \"model\": model, \n",
    "                            \"valid\": target_spdata[i],\n",
    "                            \"valid_trans\": target_train_trans,\n",
    "                            \"ref_label\": target_splabel[i][:, 1],\n",
    "                            \"repeat\": 5,\n",
    "                            \"n_bootstrap\": 100,\n",
    "                            \"njobs\": 20,\n",
    "                            \"bootstrap_tol\": 0.2,\n",
    "                            \"max_feature\": 4,\n",
    "                            \"verbose\": False,\n",
    "                        }\n",
    "                        feature_selection_process = feature_selection_sup\n",
    "\n",
    "                    elif train_mode == \"unsup\":    \n",
    "                        # unsupervised feature selection\n",
    "                        params = {\n",
    "                            \"model\": model, \n",
    "                            \"valid\": target_pseudata[i],\n",
    "                            \"valid_trans\": targert_sub_trans,\n",
    "                            \"ref_label\": source_pred_sub,\n",
    "                            \"repeat\": 5,\n",
    "                            \"delta\": 0.05,\n",
    "                            \"n_bootstrap\": 100,\n",
    "                            \"njobs\": 20,\n",
    "                            \"bootstrap_tol\": 0.2,\n",
    "                            \"max_feature\": 4,\n",
    "                            \"verbose\": False,\n",
    "                        }\n",
    "                        feature_selection_process = feature_selection\n",
    "\n",
    "                    else:\n",
    "                        print(\"Unexpected parameters\", flush=True)\n",
    "                        raise \n",
    "\n",
    "                    feature_mask = feature_selection_process(**params)\n",
    "                    fmasks.append([feature_mask])\n",
    "\n",
    "                pi = get_pi(pscore, target_sample)\n",
    "                predij = pred_ij(models, adapts, fmasks, target_sample, repeat=5, njobs=20)\n",
    "\n",
    "                pi_all = get_pi(pscore, target_train)\n",
    "                predij_all = pred_ij(models, adapts, fmasks, target_train, repeat=5, njobs=20)\n",
    "\n",
    "                sij_softmax = np.ones((kt, 1))\n",
    "\n",
    "                sij_ts[kt][ks] = sij_softmax\n",
    "\n",
    "                pred = weight_pred(pi, predij, sij_softmax)\n",
    "                pred_ts[kt][ks] = pred\n",
    "\n",
    "                pred_all = weight_pred(pi_all, predij_all, sij_softmax)\n",
    "                pred_all_ts[kt][ks] = pred_all\n",
    "\n",
    "        \n",
    "        if train_mode == \"sup\":\n",
    "            counter_weight = optimal_counter_weight_sup(pred_ts, target_sample_label[:, 1], \n",
    "                                                        n_bootstrap=10, max_iter=1000, tol=1e-5, lr=1000)\n",
    "        elif train_mode == \"unsup\":\n",
    "            counter_weight = optimal_kts_unsup_reg(pred_all_ts, source_pred, \n",
    "                                                   n_bootstrap=10, max_iter=1000, tol=1e-5, lr=1000)\n",
    "        else:\n",
    "            print(\"Unexpected parameters\", flush=True)\n",
    "            raise \n",
    "\n",
    "\n",
    "        # prediction\n",
    "        pred_final = np.zeros(target_test.shape[0])\n",
    "        for kt in counter_weight:\n",
    "            for ks in counter_weight[kt]:\n",
    "                pi = get_pi(pscore_t[kt], target_test)\n",
    "                predij = pred_ij(model_map_ts[kt][ks], adapt_map_ts[kt][ks], fmask_map_ts[kt][ks], \n",
    "                                 target_test, repeat=10, njobs=20)\n",
    "                pred = weight_pred(pi, predij, sij_ts[kt][ks])\n",
    "                pred_final += pred * counter_weight[kt][ks]\n",
    "                \n",
    "        print(\"Vote Weights:\", dict(counter_weight))\n",
    "        \n",
    "        if not test_flag:\n",
    "            save_pickle(fmask_map_ts, \"../data/results/{}/{}_{}_{}_{}_{}\".format(\n",
    "                task, model_type, version, p, seed, \"fmask_map_ts\"))\n",
    "            save_pickle(counter_weight, \"../data/results/{}/{}_{}_{}_{}_{}\".format(\n",
    "                task, model_type, version, p, seed, \"counter_weight\"))\n",
    "\n",
    "\n",
    "        perf = performance_logloss(pred_final, target_test_label[:, 1])\n",
    "        if not test_flag:\n",
    "            model_log(\"../logs/logloss/\", task, source_domain, model_type, p, source_version, \n",
    "                     \"{}: {}\".format(version, perf))\n",
    "        print(\"Target Prediction logloss kt={}, ks={}\".format(\"All\", \"All\"), perf, flush=True)\n",
    "        \n",
    "        \n",
    "        perf = performance_pr_auc(pred_final, target_test_label[:, 1])\n",
    "        if not test_flag:\n",
    "            model_log(\"../logs/pr_auc/\", task, source_domain, model_type, p, source_version, \n",
    "                     \"{}: {}\".format(version, perf))\n",
    "        print(\"Target Prediction pr_auc kt={}, ks={}\".format(\"All\", \"All\"), perf, flush=True)\n",
    "\n",
    "\n",
    "        # no subdomains\n",
    "        best_kt = 1\n",
    "        best_ks = 1\n",
    "        pi = get_pi(pscore_t[best_kt], target_test)\n",
    "        predij = pred_ij(model_map_ts[best_kt][best_ks], adapt_map_ts[best_kt][best_ks], fmask_map_ts[best_kt][best_ks], \n",
    "                         target_test, repeat=10, njobs=20)\n",
    "        pred = weight_pred(pi, predij, sij_ts[best_kt][best_ks])\n",
    "        \n",
    "        # performance evaluation on target test for logloss\n",
    "        perf = performance_logloss(pred, target_test_label[:, 1])\n",
    "        if not test_flag:\n",
    "            model_log(\"../logs/logloss/\", task, source_domain, model_type, p, source_version, \n",
    "                     \"{}: {}\".format(basic_version, perf))\n",
    "        print(\"Target Prediction logloss kt={}, ks={}\".format(best_kt, best_ks), perf, flush=True)\n",
    "\n",
    "        perf = performance_pr_auc(pred, target_test_label[:, 1])\n",
    "        if not test_flag:\n",
    "            model_log(\"../logs/pr_auc/\", task, source_domain, model_type, p, source_version, \n",
    "                     \"{}: {}\".format(basic_version, perf))\n",
    "        print(\"Target Prediction pr_auc kt={}, ks={}\".format(best_kt, best_ks), perf, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
